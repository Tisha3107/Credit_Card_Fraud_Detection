{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f7f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import streamlit as st\n",
    "import time\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901bc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_data():\n",
    "    try:\n",
    "        # Attempt to load from file if available\n",
    "        df = pd.read_csv('creditcard.csv')\n",
    "    except:\n",
    "        # If file not available, notify user\n",
    "        st.error(\"Please upload the creditcard.csv file to continue\")\n",
    "        df = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9e529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample dataset\n",
    "def load_sample_data():\n",
    "    # Create a simplified sample dataset with similar structure\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    n_features = 28\n",
    "    \n",
    "    # Generate features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Generate some anomalies (0.5% fraud)\n",
    "    n_frauds = int(0.005 * n_samples)\n",
    "    \n",
    "    # For fraudulent transactions, modify some features to be outliers\n",
    "    for i in range(n_frauds):\n",
    "        X[i, np.random.randint(0, n_features, 3)] += np.random.randn(3) * 5\n",
    "    \n",
    "    # Create class labels (0 for normal, 1 for fraud)\n",
    "    y = np.zeros(n_samples)\n",
    "    y[:n_frauds] = 1\n",
    "    \n",
    "    # Create a DataFrame with similar structure to creditcard.csv\n",
    "    columns = ['V' + str(i+1) for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    \n",
    "    # Add Time, Amount, and Class columns\n",
    "    df['Time'] = np.random.randint(0, 172800, n_samples)  # Seconds in 2 days\n",
    "    df['Amount'] = np.abs(np.random.randn(n_samples) * 100 + 50)  # Transaction amounts\n",
    "    df['Class'] = y\n",
    "    \n",
    "    # Make fraudulent transactions have slightly higher amounts\n",
    "    df.loc[df['Class'] == 1, 'Amount'] *= 1.5\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f1408a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    \n",
    "    # Check the distribution of the 'Class' variable\n",
    "    class_distribution = df['Class'].value_counts()\n",
    "    \n",
    "    # Separate features from target\n",
    "    X = df.drop(['Class', 'Amount', 'Time'], axis=1)\n",
    "    \n",
    "    # Scale the Amount feature\n",
    "    amount = df['Amount'].values.reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    df['Amount_scaled'] = scaler.fit_transform(amount)\n",
    "    \n",
    "    # Scale the Time feature\n",
    "    time_feature = df['Time'].values.reshape(-1, 1)\n",
    "    df['Time_scaled'] = scaler.fit_transform(time_feature)\n",
    "    \n",
    "    # Prepare scaled dataset with all features\n",
    "    X_scaled = df.drop(['Class', 'Amount', 'Time'], axis=1)\n",
    "    X_scaled['Amount_scaled'] = df['Amount_scaled']\n",
    "    X_scaled['Time_scaled'] = df['Time_scaled']\n",
    "    \n",
    "    return X_scaled, df['Class'], missing_values, class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd55e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dimensionality Reduction with PCA for Visualization\n",
    "def apply_pca(X_scaled):\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(X_scaled)\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    return pca_df, pca, explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac933b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Unsupervised Learning Models\n",
    "# 4.1 Isolation Forest\n",
    "def isolation_forest_model(X_scaled, contamination=0.01):\n",
    "    model = IsolationForest(contamination=contamination, random_state=42)\n",
    "    model.fit(X_scaled)\n",
    "    # Prediction: 1 for inliers, -1 for outliers\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    # Convert to binary: 0 for inliers, 1 for outliers (frauds)\n",
    "    y_pred = [1 if pred == -1 else 0 for pred in y_pred]\n",
    "    return y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d321a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to calculate feature importance for Isolation Forest\n",
    "def calculate_feature_importance(model, X):\n",
    "    \"\"\"\n",
    "    Calculate feature importance for Isolation Forest\n",
    "    \n",
    "    Instead of relying on the feature_importances_ attribute,\n",
    "    we'll calculate importance based on the decision path depths\n",
    "    \"\"\"\n",
    "    # For each feature, we'll measure how much it contributes to anomaly detection\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Get the decision paths\n",
    "    # We'll use decision_function as a proxy for importance\n",
    "    anomaly_scores = -model.decision_function(X)  # Higher score = more anomalous\n",
    "    \n",
    "    # Calculate feature importance by correlation with anomaly scores\n",
    "    importance = np.zeros(n_features)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        # Calculate correlation between feature and anomaly score\n",
    "        corr = np.corrcoef(X.iloc[:, i], anomaly_scores)[0, 1]\n",
    "        importance[i] = np.abs(corr)  # Take absolute value of correlation\n",
    "    \n",
    "    # Normalize importance\n",
    "    if np.sum(importance) > 0:\n",
    "        importance = importance / np.sum(importance)\n",
    "        \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e622ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Local Outlier Factor\n",
    "def local_outlier_factor(X_scaled, contamination=0.01):\n",
    "    model = LocalOutlierFactor(n_neighbors=20, contamination=contamination)\n",
    "    # Prediction: 1 for inliers, -1 for outliers\n",
    "    y_pred = model.fit_predict(X_scaled)\n",
    "    # Convert to binary: 0 for inliers, 1 for outliers (frauds)\n",
    "    y_pred = [1 if pred == -1 else 0 for pred in y_pred]\n",
    "    return y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f51227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 DBSCAN\n",
    "def dbscan_model(X_scaled, eps=0.3, min_samples=10):\n",
    "    model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    y_pred = model.fit_predict(X_scaled)\n",
    "    # In DBSCAN, -1 represents outliers\n",
    "    y_pred = [1 if pred == -1 else 0 for pred in y_pred]\n",
    "    return y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50540e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Evaluation\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1  # Key is 'F1', not 'F1 Score'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c1557bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualization Functions\n",
    "def plot_pca_results(pca_df, y_true, y_pred=None):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    if y_pred is not None:\n",
    "        # Plot with predicted labels\n",
    "        plt.scatter(pca_df['PC1'], pca_df['PC2'], c=y_pred, cmap='coolwarm', alpha=0.7)\n",
    "        plt.title('PCA of Credit Card Transactions with Predicted Fraud')\n",
    "    else:\n",
    "        # Plot with actual labels\n",
    "        plt.scatter(pca_df['PC1'], pca_df['PC2'], c=y_true, cmap='coolwarm', alpha=0.7)\n",
    "        plt.title('PCA of Credit Card Transactions with Actual Fraud')\n",
    "    \n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.colorbar(label='Class')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26d9585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a downloadable report\n",
    "def generate_report(df, X_scaled, y_true, y_pred, model_choice, results, missing_values, class_distribution):\n",
    "    report = io.StringIO()\n",
    "    \n",
    "    report.write(\"# Credit Card Fraud Detection Report\\n\\n\")\n",
    "    report.write(f\"## Dataset Overview\\n\")\n",
    "    report.write(f\"- Total transactions: {len(df)}\\n\")\n",
    "    report.write(f\"- Normal transactions: {sum(y_true == 0)}\\n\")\n",
    "    report.write(f\"- Fraudulent transactions: {sum(y_true == 1)}\\n\")\n",
    "    report.write(f\"- Fraud percentage: {sum(y_true == 1)/len(y_true)*100:.2f}%\\n\\n\")\n",
    "    \n",
    "    report.write(f\"## Data Quality\\n\")\n",
    "    report.write(f\"- Missing values: {sum(missing_values)}\\n\\n\")\n",
    "    \n",
    "    report.write(f\"## Model: {model_choice}\\n\")\n",
    "    report.write(f\"- Accuracy: {results['Accuracy']:.4f}\\n\")\n",
    "    report.write(f\"- Precision: {results['Precision']:.4f}\\n\")\n",
    "    report.write(f\"- Recall: {results['Recall']:.4f}\\n\")\n",
    "    report.write(f\"- F1 Score: {results['F1']:.4f}\\n\\n\")\n",
    "    \n",
    "    report.write(f\"## Prediction Results\\n\")\n",
    "    report.write(f\"- True Positives: {sum((y_true == 1) & (np.array(y_pred) == 1))}\\n\")\n",
    "    report.write(f\"- False Positives: {sum((y_true == 0) & (np.array(y_pred) == 1))}\\n\")\n",
    "    report.write(f\"- True Negatives: {sum((y_true == 0) & (np.array(y_pred) == 0))}\\n\")\n",
    "    report.write(f\"- False Negatives: {sum((y_true == 1) & (np.array(y_pred) == 0))}\\n\\n\")\n",
    "    \n",
    "    return report.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99314bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Streamlit Application\n",
    "def create_streamlit_app():\n",
    "    st.set_page_config(page_title=\"Credit Card Fraud Detection\", page_icon=\"üí≥\", layout=\"wide\")\n",
    "    \n",
    "    st.title(\"Credit Card Fraud Detection\")\n",
    "    st.markdown(\"\"\"\n",
    "    This application uses unsupervised machine learning to detect fraudulent credit card transactions.\n",
    "    You can upload your own dataset or use a sample dataset to get started.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Sidebar for data selection options\n",
    "    st.sidebar.title(\"Data Options\")\n",
    "    data_option = st.sidebar.radio(\n",
    "        \"Select data source\",\n",
    "        [\"Upload your own dataset\", \"Use sample dataset\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize df as None\n",
    "    df = None\n",
    "    \n",
    "    # Handle data selection\n",
    "    if data_option == \"Upload your own dataset\":\n",
    "        # File uploader\n",
    "        uploaded_file = st.file_uploader(\"Upload creditcard.csv\", type=[\"csv\"])\n",
    "        \n",
    "        if uploaded_file is not None:\n",
    "            # Load data\n",
    "            df = pd.read_csv(uploaded_file)\n",
    "            st.success(\"File uploaded successfully!\")\n",
    "    else:\n",
    "        # Use sample data\n",
    "        if st.button(\"Load Sample Dataset\"):\n",
    "            with st.spinner(\"Generating sample dataset...\"):\n",
    "                df = load_sample_data()\n",
    "                st.success(\"Sample dataset loaded successfully!\")\n",
    "    \n",
    "    # Only continue if we have data\n",
    "    if df is not None:\n",
    "        # Display raw data overview\n",
    "        with st.expander(\"View Raw Data Preview\"):\n",
    "            st.dataframe(df.head())\n",
    "            st.write(f\"Dataset Shape: {df.shape}\")\n",
    "        \n",
    "        # Data preprocessing\n",
    "        X_scaled, y_true, missing_values, class_distribution = preprocess_data(df)\n",
    "        \n",
    "        # Show data statistics\n",
    "        with st.expander(\"Data Statistics\"):\n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                st.write(\"Feature Statistics:\")\n",
    "                st.dataframe(X_scaled.describe())\n",
    "            \n",
    "            with col2:\n",
    "                # Class distribution chart\n",
    "                fig = px.pie(names=['Normal', 'Fraud'], \n",
    "                             values=df['Class'].value_counts().values, \n",
    "                             title='Transaction Class Distribution')\n",
    "                st.plotly_chart(fig)\n",
    "                \n",
    "                st.info(\"\"\"\n",
    "                **What does this chart mean?**\n",
    "                \n",
    "                This pie chart shows the distribution of normal vs. fraudulent transactions in the dataset.\n",
    "                Credit card fraud datasets are typically highly imbalanced, with fraudulent transactions\n",
    "                representing a very small percentage of the total transactions.\n",
    "                \"\"\")\n",
    "        \n",
    "        # Feature exploration\n",
    "        with st.expander(\"Feature Exploration\"):\n",
    "            selected_features = st.multiselect(\n",
    "                \"Select features to explore\",\n",
    "                X_scaled.columns.tolist(),\n",
    "                default=X_scaled.columns[:2].tolist()\n",
    "            )\n",
    "            \n",
    "            if selected_features:\n",
    "                # Create distributions by class\n",
    "                fig = go.Figure()\n",
    "                \n",
    "                for feature in selected_features:\n",
    "                    fig.add_trace(go.Violin(\n",
    "                        x=['Normal']*sum(y_true == 0),\n",
    "                        y=X_scaled[feature][y_true == 0],\n",
    "                        name=f'{feature} - Normal',\n",
    "                        box_visible=True,\n",
    "                        meanline_visible=True,\n",
    "                        side='negative',\n",
    "                        line_color='blue'\n",
    "                    ))\n",
    "                    \n",
    "                    fig.add_trace(go.Violin(\n",
    "                        x=['Fraud']*sum(y_true == 1),\n",
    "                        y=X_scaled[feature][y_true == 1],\n",
    "                        name=f'{feature} - Fraud',\n",
    "                        box_visible=True,\n",
    "                        meanline_visible=True,\n",
    "                        side='positive',\n",
    "                        line_color='red'\n",
    "                    ))\n",
    "                \n",
    "                fig.update_layout(\n",
    "                    title=\"Distribution of Selected Features by Class\",\n",
    "                    xaxis_title=\"Transaction Class\",\n",
    "                    yaxis_title=\"Feature Value\",\n",
    "                    violinmode='overlay'\n",
    "                )\n",
    "                \n",
    "                st.plotly_chart(fig)\n",
    "                \n",
    "                st.info(\"\"\"\n",
    "                **What does this chart mean?**\n",
    "                \n",
    "                These violin plots show the distribution of feature values for normal vs. fraudulent transactions.\n",
    "                Significant differences in distributions indicate that the feature might be useful for detecting fraud.\n",
    "                The wider sections of the violin plot represent a higher probability of observations taking that value.\n",
    "                \"\"\")\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca_df, pca, explained_variance = apply_pca(X_scaled)\n",
    "        \n",
    "        # Model selection sidebar\n",
    "        st.sidebar.title(\"Model Configuration\")\n",
    "        model_choice = st.sidebar.selectbox(\n",
    "            \"Select Unsupervised Learning Model\",\n",
    "            [\"Isolation Forest\", \"Local Outlier Factor\", \"DBSCAN\"]\n",
    "        )\n",
    "        \n",
    "        # Add model explanations in the sidebar\n",
    "        if model_choice == \"Isolation Forest\":\n",
    "            st.sidebar.info(\"\"\"\n",
    "            **Isolation Forest** works by isolating observations by randomly selecting a feature \n",
    "            and then randomly selecting a split value between the maximum and minimum values of \n",
    "            the selected feature. Since isolating anomalies is easier (fewer splits needed), \n",
    "            the algorithm can identify them with shorter paths in the decision tree.\n",
    "            \"\"\")\n",
    "            contamination = st.sidebar.slider(\"Contamination (expected % of outliers)\", 0.001, 0.1, 0.01, 0.001)\n",
    "            \n",
    "            with st.spinner('Training Isolation Forest model...'):\n",
    "                y_pred, model = isolation_forest_model(X_scaled, contamination)\n",
    "                \n",
    "        elif model_choice == \"Local Outlier Factor\":\n",
    "            st.sidebar.info(\"\"\"\n",
    "            **Local Outlier Factor (LOF)** measures the local deviation of density of a given sample \n",
    "            with respect to its neighbors. It detects outliers by finding data points that have a \n",
    "            substantially lower density than their neighbors.\n",
    "            \"\"\")\n",
    "            contamination = st.sidebar.slider(\"Contamination (expected % of outliers)\", 0.001, 0.1, 0.01, 0.001)\n",
    "            n_neighbors = st.sidebar.slider(\"Number of Neighbors\", 5, 50, 20)\n",
    "            \n",
    "            with st.spinner('Training Local Outlier Factor model...'):\n",
    "                model = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "                y_pred = model.fit_predict(X_scaled.values)\n",
    "                y_pred = [1 if pred == -1 else 0 for pred in y_pred]\n",
    "                \n",
    "        elif model_choice == \"DBSCAN\":\n",
    "            st.sidebar.info(\"\"\"\n",
    "            **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** groups \n",
    "            together points that are closely packed together, marking as outliers points that \n",
    "            lie alone in low-density regions. It's effective for data without a clear cluster structure.\n",
    "            \"\"\")\n",
    "            eps = st.sidebar.slider(\"Epsilon (neighborhood distance)\", 0.1, 5.0, 0.5, 0.1)\n",
    "            min_samples = st.sidebar.slider(\"Min Samples (min points in neighborhood)\", 5, 100, 10)\n",
    "            \n",
    "            with st.spinner('Training DBSCAN model...'):\n",
    "                y_pred, model = dbscan_model(X_scaled, eps, min_samples)\n",
    "        \n",
    "        # Model evaluation\n",
    "        results = evaluate_model(y_true, y_pred)\n",
    "        \n",
    "        # Display results\n",
    "        st.header(\"Model Results\")\n",
    "        \n",
    "        \n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        col1.metric(\"Accuracy\", f\"{results['Accuracy']:.4f}\")\n",
    "        col2.metric(\"Precision\", f\"{results['Precision']:.4f}\")\n",
    "        col3.metric(\"Recall\", f\"{results['Recall']:.4f}\")\n",
    "        col4.metric(\"F1 Score\", f\"{results['F1']:.4f}\")  # Key is 'F1'\n",
    "\n",
    "        # Add Fraud Detection Summary right after metrics\n",
    "        st.subheader(\"üìä Fraud Detection Summary\")\n",
    "\n",
    "        # Create summary statistics\n",
    "        total_transactions = len(y_true)\n",
    "        actual_frauds = sum(y_true)\n",
    "        predicted_frauds = sum(y_pred)\n",
    "        correct_fraud_predictions = sum((y_true == 1) & (np.array(y_pred) == 1))\n",
    "        missed_frauds = sum((y_true == 1) & (np.array(y_pred) == 0))\n",
    "        false_alarms = sum((y_true == 0) & (np.array(y_pred) == 1))\n",
    "\n",
    "        # Display summary statistics in two columns\n",
    "        col1, col2 = st.columns(2)\n",
    "\n",
    "        with col1:\n",
    "            st.markdown(f\"\"\"\n",
    "            **Transaction Analysis**:\n",
    "            - Total Transactions: {total_transactions:,}\n",
    "            - Actual Fraudulent Transactions: {actual_frauds:,} ({actual_frauds/total_transactions:.2%})\n",
    "            - Predicted Fraudulent Transactions: {predicted_frauds:,} ({predicted_frauds/total_transactions:.2%})\n",
    "            \"\"\")\n",
    "\n",
    "        with col2:\n",
    "            st.markdown(f\"\"\"\n",
    "            **Fraud Detection Results**:\n",
    "            - Correctly Identified Frauds: {correct_fraud_predictions:,} ({correct_fraud_predictions/actual_frauds:.2%} of all frauds)\n",
    "            - Missed Frauds: {missed_frauds:,} ({missed_frauds/actual_frauds:.2%} of all frauds)\n",
    "            - False Alarms: {false_alarms:,} ({false_alarms/predicted_frauds:.2%} of fraud predictions)\n",
    "            \"\"\")\n",
    "\n",
    "        # Add a visual indicator of overall effectiveness\n",
    "        fraud_detection_rate = correct_fraud_predictions/actual_frauds if actual_frauds > 0 else 0\n",
    "        effectiveness_color = \"red\" if fraud_detection_rate < 0.5 else \"orange\" if fraud_detection_rate < 0.8 else \"green\"\n",
    "\n",
    "        st.markdown(f\"\"\"\n",
    "        <div style='background-color: {effectiveness_color}; padding: 10px; border-radius: 5px; margin-top: 10px; margin-bottom: 20px;'>\n",
    "            <h3 style='color: white; margin: 0;'>Detection Effectiveness: {fraud_detection_rate:.2%}</h3>\n",
    "            <p style='color: white; margin: 0;'>\n",
    "                {\n",
    "                \"‚ö†Ô∏è POOR - The majority of frauds are being missed!\" if fraud_detection_rate < 0.5 else\n",
    "                \"‚ö†Ô∏è MODERATE - A significant number of frauds are still being missed.\" if fraud_detection_rate < 0.8 else\n",
    "                \"‚úÖ GOOD - Most fraudulent transactions are being detected.\"\n",
    "                }\n",
    "            </p>\n",
    "        </div>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "        with st.expander(\"What do these metrics mean?\"):\n",
    "            st.markdown(\"\"\"\n",
    "            **Accuracy**: The proportion of correctly classified transactions (both normal and fraudulent).\n",
    "    \n",
    "            **Precision**: The proportion of predicted frauds that are actually fraudulent. Higher precision means fewer false alarms.\n",
    "    \n",
    "            **Recall**: The proportion of actual frauds that were correctly identified. Higher recall means fewer missed frauds.\n",
    "    \n",
    "            **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "    \n",
    "            In fraud detection, recall is often more important than precision, as the cost of missing a fraudulent transaction (false negative) is typically higher than the cost of investigating a legitimate transaction (false positive).\n",
    "            \"\"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Confusion Matrix\n",
    "        st.subheader(\"Confusion Matrix\")\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "                z=cm,\n",
    "                x=['Predicted Normal', 'Predicted Fraud'],\n",
    "                y=['Actual Normal', 'Actual Fraud'],\n",
    "                hoverongaps=False,\n",
    "                colorscale='Viridis',\n",
    "                text=cm,\n",
    "                texttemplate=\"%{text}\"))\n",
    "        \n",
    "        fig.update_layout(title=\"Confusion Matrix\")\n",
    "        st.plotly_chart(fig)\n",
    "        \n",
    "        with st.expander(\"What does this confusion matrix mean?\"):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            st.markdown(f\"\"\"\n",
    "            **True Negatives ({tn})**: Normal transactions correctly identified as normal.\n",
    "            \n",
    "            **False Positives ({fp})**: Normal transactions incorrectly flagged as fraudulent.\n",
    "            \n",
    "            **False Negatives ({fn})**: Fraudulent transactions missed by the model.\n",
    "            \n",
    "            **True Positives ({tp})**: Fraudulent transactions correctly identified as fraudulent.\n",
    "            \n",
    "            In fraud detection, minimizing False Negatives (missed frauds) is typically more important than minimizing False Positives (false alarms).\n",
    "            \"\"\")\n",
    "        \n",
    "        # PCA Visualization\n",
    "        st.subheader(\"PCA Visualization\")\n",
    "        \n",
    "        # Explain PCA\n",
    "        with st.expander(\"What is PCA and how to interpret this visualization?\"):\n",
    "            st.markdown(f\"\"\"\n",
    "            **Principal Component Analysis (PCA)** is a dimensionality reduction technique that transforms the data into a new coordinate system.\n",
    "            \n",
    "            In this visualization:\n",
    "            - Each point represents a transaction\n",
    "            - The x-axis (PC1) and y-axis (PC2) are the two principal components that capture the most variance in the data\n",
    "            - PC1 explains {explained_variance[0]*100:.2f}% of the variance\n",
    "            - PC2 explains {explained_variance[1]*100:.2f}% of the variance\n",
    "            - Together they explain {sum(explained_variance)*100:.2f}% of the total variance\n",
    "            \n",
    "            If fraudulent transactions form distinct clusters in this visualization, it suggests that the model can effectively separate them from normal transactions.\n",
    "            \"\"\")\n",
    "        \n",
    "        # Create a dataframe with PCA components and class\n",
    "        pca_viz_df = pd.DataFrame({\n",
    "            'PC1': pca_df['PC1'],\n",
    "            'PC2': pca_df['PC2'],\n",
    "            'Actual Class': y_true,\n",
    "            'Predicted Class': y_pred\n",
    "        })\n",
    "        \n",
    "        # Create tabs for different visualizations\n",
    "        tab1, tab2 = st.tabs([\"Actual vs. Predicted\", \"Detailed View\"])\n",
    "        \n",
    "        with tab1:\n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                fig1 = px.scatter(pca_viz_df, x='PC1', y='PC2', color='Actual Class',\n",
    "                              title='PCA with Actual Fraud Labels',\n",
    "                              color_continuous_scale='Viridis')\n",
    "                st.plotly_chart(fig1)\n",
    "                \n",
    "            with col2:\n",
    "                fig2 = px.scatter(pca_viz_df, x='PC1', y='PC2', color='Predicted Class',\n",
    "                              title='PCA with Predicted Fraud Labels',\n",
    "                              color_continuous_scale='Viridis')\n",
    "                st.plotly_chart(fig2)\n",
    "        \n",
    "        with tab2:\n",
    "            # Interactive scatter plot\n",
    "            st.subheader(\"Interactive PCA Plot\")\n",
    "            fig = px.scatter(\n",
    "                pca_viz_df, x='PC1', y='PC2',\n",
    "                color='Predicted Class',\n",
    "                hover_data=['Actual Class'],\n",
    "                title='PCA of Credit Card Transactions',\n",
    "                color_continuous_scale='Viridis'\n",
    "            )\n",
    "            st.plotly_chart(fig)\n",
    "        \n",
    "        # Model-specific visualizations\n",
    "        st.header(f\"{model_choice} Analysis\")\n",
    "        \n",
    "        if model_choice == \"Isolation Forest\":\n",
    "            # Feature importance for Isolation Forest using our custom function\n",
    "            st.subheader(\"Feature Importance\")\n",
    "            \n",
    "            # Calculate feature importance using our custom function\n",
    "            try:\n",
    "                # Use our custom function to calculate feature importance\n",
    "                importances = calculate_feature_importance(model, X_scaled)\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                features = X_scaled.columns\n",
    "                \n",
    "                fig = go.Figure(go.Bar(\n",
    "                    x=[features[i] for i in indices],\n",
    "                    y=importances[indices],\n",
    "                    marker_color='green'\n",
    "                ))\n",
    "                fig.update_layout(title=\"Feature Importance in Isolation Forest\",\n",
    "                                 xaxis_title=\"Features\",\n",
    "                                 yaxis_title=\"Importance Score\")\n",
    "                st.plotly_chart(fig)\n",
    "                \n",
    "                with st.expander(\"How to interpret feature importance?\"):\n",
    "                    st.markdown(\"\"\"\n",
    "                    **Feature importance** indicates which features are most useful for detecting fraud. \n",
    "                    \n",
    "                    Features with higher importance scores have a stronger influence on the model's decisions.\n",
    "                    In fraud detection, these might represent transaction characteristics that frequently differ\n",
    "                    between legitimate and fraudulent activity.\n",
    "                    \n",
    "                    Paying attention to these features can help in:\n",
    "                    - Understanding the patterns of fraudulent activity\n",
    "                    - Creating more targeted fraud detection rules\n",
    "                    - Focusing investigation efforts on the most suspicious aspects of transactions\n",
    "                    \"\"\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.warning(f\"Could not calculate feature importance. Details: {str(e)}\")\n",
    "                st.info(\"Showing feature distribution instead.\")\n",
    "                \n",
    "                # Show feature distributions as an alternative\n",
    "                fig = px.box(X_scaled, title=\"Feature Distributions\")\n",
    "                st.plotly_chart(fig)\n",
    "            \n",
    "        elif model_choice in [\"Local Outlier Factor\", \"DBSCAN\"]:\n",
    "            # Show anomaly scores distribution\n",
    "            st.subheader(\"Transaction Distribution\")\n",
    "            \n",
    "            # Create a figure with outliers highlighted\n",
    "            fig = px.scatter(\n",
    "                x=range(len(y_pred)),\n",
    "                y=X_scaled.iloc[:, 0],  # Using first feature as y-axis\n",
    "                color=[('Fraud' if p == 1 else 'Normal') for p in y_pred],\n",
    "                title=f\"Transactions with {model_choice} Outliers Highlighted\",\n",
    "                labels={\"x\": \"Transaction Index\", \"y\": \"Feature V1\"},\n",
    "                color_discrete_map={\"Normal\": \"blue\", \"Fraud\": \"red\"}\n",
    "            )\n",
    "            st.plotly_chart(fig)\n",
    "            \n",
    "            with st.expander(f\"How {model_choice} detects outliers\"):\n",
    "                if model_choice == \"Local Outlier Factor\":\n",
    "                    st.markdown(\"\"\"\n",
    "                    **Local Outlier Factor (LOF)** calculates the local density deviation of a data point with respect to its neighbors. \n",
    "                    \n",
    "                    Points with significantly lower density than their neighbors are considered outliers (potential frauds).\n",
    "                    \n",
    "                    In this visualization, red points represent transactions that the model considers anomalous based on their\n",
    "                    difference from surrounding transactions in the feature space.\n",
    "                    \"\"\")\n",
    "                else:  # DBSCAN\n",
    "                    st.markdown(\"\"\"\n",
    "                    **DBSCAN** identifies clusters of transactions with similar characteristics. \n",
    "                    \n",
    "                    Transactions that don't belong to any cluster (aren't similar enough to a sufficient number of other transactions)\n",
    "                    are flagged as potential frauds.\n",
    "                    \n",
    "                    In this visualization, red points represent transactions that don't fit into any of the identified clusters\n",
    "                    of normal transaction patterns.\n",
    "                    \"\"\")\n",
    "        \n",
    "        # Generate detailed report\n",
    "        st.header(\"Analysis Report\")\n",
    "        report_content = generate_report(df, X_scaled, y_true, y_pred, model_choice, results, missing_values, class_distribution)\n",
    "        \n",
    "        st.download_button(\n",
    "            label=\"Download Full Analysis Report\",\n",
    "            data=report_content,\n",
    "            file_name=\"fraud_detection_report.md\",\n",
    "            mime=\"text/markdown\"\n",
    "        )\n",
    "        \n",
    "        # Download predictions\n",
    "        prediction_df = pd.DataFrame({\n",
    "            'Transaction_ID': range(len(y_true)),\n",
    "            'Actual': y_true,\n",
    "            'Predicted': y_pred\n",
    "        })\n",
    "        \n",
    "        st.download_button(\n",
    "            label=\"Download Predictions\",\n",
    "            data=prediction_df.to_csv(index=False),\n",
    "            file_name=\"fraud_predictions.csv\",\n",
    "            mime=\"text/csv\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb0ee630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 20:35:53.378 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:53.382 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.486 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Tisha Verma\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-03-30 20:35:55.489 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.492 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.493 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.495 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.497 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.498 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.508 Session state does not function when running a script without `streamlit run`\n",
      "2025-03-30 20:35:55.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.512 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.516 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 20:35:55.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Run the main Streamlit app\n",
    "if __name__ == \"__main__\":\n",
    "    create_streamlit_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0abef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
